{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62564008",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd8e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, DefaultDict, Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import ExplicitDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2aea77",
   "metadata": {},
   "source": [
    "#### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   user_id  100836 non-null  int64  \n",
      " 1   item_id  100836 non-null  int64  \n",
      " 2   rating   100836 non-null  float64\n",
      " 3   ts       100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n",
      "\n",
      "\n",
      "    n_users  n_items  n_ratings\n",
      "1      610     9724     100836\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/ml-latest-small/ratings.csv\")\n",
    "df.rename(\n",
    "    columns={\"userId\": \"user_id\", \"movieId\": \"item_id\", \"timestamp\": \"ts\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c667173",
   "metadata": {},
   "source": [
    "#### **Predict Ratings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "919b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=32):\n",
    "        \"\"\"\n",
    "        emb_dim = 32\n",
    "\n",
    "        item and user embeddings get concatenated, resulting in an embedding with 64d.\n",
    "\n",
    "        (original paper)\n",
    "        layers (3): 64d - 32d - 16d (2 hidden layers + output layer / last hidden layer)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # learnable parameters - user and item embedding matrices\n",
    "        # user embedding matrix size = n_users x emb_dim\n",
    "        # item embedding matrix size = n_items x emb_dim\n",
    "        self.user_embedding = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, emb_dim)\n",
    "\n",
    "        # single linear layer: 64 -> 1\n",
    "        self.output = nn.Linear(2 * emb_dim, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Zero hidden layers.\n",
    "\n",
    "        All it does: for the 64d concatenated embedding, it outputs a single value that\n",
    "        passes through a linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        u_emb = self.user_embedding(user_ids)  # size: [batch, 32]\n",
    "        i_emb = self.item_embedding(item_ids)  # size: [batch, 32]\n",
    "\n",
    "        x = torch.cat([u_emb, i_emb], dim=1)\n",
    "        output = self.output(x)\n",
    "\n",
    "        print(type(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1fff8",
   "metadata": {},
   "source": [
    "**Datasets construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76bf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the user and item id to start from 0 (this is what nn.Embedding expects)\n",
    "# this prevents us from run into index out of bound \"error\" with Embedding lookup\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_item = preprocessing.LabelEncoder()\n",
    "\n",
    "df.user_id = lbl_user.fit_transform(df.user_id.values)\n",
    "df.item_id = lbl_item.fit_transform(df.item_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5553ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: train set = 90752; test set = 10084\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.rating.values\n",
    ")\n",
    "\n",
    "train_dataset = ExplicitDataset(\n",
    "    users=df_train.user_id.values,\n",
    "    items=df_train.item_id.values,\n",
    "    targets=df_train.rating.values,\n",
    ")\n",
    "\n",
    "test_dataset = ExplicitDataset(\n",
    "    users=df_test.user_id.values,\n",
    "    items=df_test.item_id.values,\n",
    "    targets=df_test.rating.values,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Lengths: train set = {}; test set = {}\".format(\n",
    "        len(train_dataset), len(test_dataset)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3c31df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor([566, 327, 413, 110]), 'items': tensor([9204, 7001, 3407, 6517]), 'targets': tensor([0.5000, 4.0000, 5.0000, 3.0000])}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "total number of batches = nb. training points / batch_size\n",
    "\"\"\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "print(next(dataiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cdc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(lbl_user.classes_)\n",
    "n_items = len(lbl_item.classes_)\n",
    "\n",
    "model = SimpleNCF(n_users=n_users, n_items=n_items).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Every `step_size` calls to scheduler.step(), multiply the learning rate by `gamma`\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ae4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 610; n_items: 9724 \n",
      "\n",
      "torch.Size([610, 32])\n",
      "torch.Size([9724, 32])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nParameters\\n----------\\nuser_matrix_embedding.weight  torch.Size([n_users, 32])\\nitem_matrix_embedding.weight  torch.Size([n_items, 32])\\noutput.weight                 torch.Size([1, 64])\\noutput.bias                   torch.Size([1])\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n_users: {}; n_items: {} \\n\".format(n_users, n_items))\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.shape)\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "----------\n",
    "user_matrix_embedding.weight  torch.Size([n_users, 32])\n",
    "item_matrix_embedding.weight  torch.Size([n_items, 32])\n",
    "output.weight                 torch.Size([1, 64])\n",
    "output.bias                   torch.Size([1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdfaf6",
   "metadata": {},
   "source": [
    "**Forward Pass Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b08077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d927791",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.5596],\n",
      "        [ 0.3537],\n",
      "        [ 0.5270],\n",
      "        [-0.8871]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 1  # nb. of times we go through the train set\n",
    "total_loss = 0\n",
    "plot_steps, print_steps = 5000, 5000\n",
    "step_cnt = 0\n",
    "all_losses_list = []\n",
    "\n",
    "model.train()\n",
    "\"\"\"\n",
    "- Puts the model into \"training mode\"\n",
    "\n",
    "- It changes how some layers behave:\n",
    "    1. Dropout layers (nn.Dropout)\n",
    "        - In train() mode: randomly zero out some activations (adds noise, regularizes).\n",
    "        - In eval() mode: no dropout, they pass everything through (but scaled \n",
    "        appropriately during training).\n",
    "\n",
    "    2. BatchNorm layers (nn.BatchNorm1d, nn.BatchNorm2d, etc.)\n",
    "        (it fixes the \"internal covariate shift\" problem)\n",
    "        - In train() mode: use the current batch's mean/variance and update running stats.\n",
    "        - In eval() mode: use the stored running mean/variance (fixed statistics).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        # move data to device\n",
    "        users = train_data[\"users\"].to(device)\n",
    "        items = train_data[\"items\"].to(device)\n",
    "        targets = train_data[\"targets\"].to(device)\n",
    "\n",
    "        # foward pass\n",
    "        pred_target = model(users, items)\n",
    "\n",
    "        print(pred_target)\n",
    "\n",
    "        true_target = targets.view(targets.size(0), -1).to(torch.float32)\n",
    "        loss = loss_func(pred_target, true_target)\n",
    "\n",
    "        # clears old gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backpropagation: performs backward propragation\n",
    "        # (fills param.grad for every parameter in model.parameters())\n",
    "        loss.backward()\n",
    "        # param update: uses the gradients in param.grads to update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # for plot purposes\n",
    "        step_cnt += len(train_data[\"users\"])\n",
    "        total_loss += loss.sum().item()\n",
    "        if step_cnt % plot_steps == 0:\n",
    "            avg_loss = total_loss / (len(train_data[\"users\"]) * plot_steps)\n",
    "            print(\"epoch {} loss at step {} os {}\".format(epoch_i, step_cnt, avg_loss))\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0  # reset total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe78c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad1f0e",
   "metadata": {},
   "source": [
    "**Evaluation Loop w/ RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    mode: Literal[\"per_sample\", \"batch_mean\"] = \"per_sample\",\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute RMSE on a test set using either per-sample or per-batch-mean aggregation.\n",
    "\n",
    "    Each batch from `test_loader` is expected to be a dict with keys:\n",
    "        - \"users\": user IDs (ignored for the metric)\n",
    "        - \"items\": item IDs (ignored for the metric)\n",
    "        - \"targets\": true ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Trained PyTorch model that takes (users, items) and outputs predictions.\n",
    "    test_loader\n",
    "        DataLoader yielding batches as dicts with \"users\", \"items\", \"targets\".\n",
    "    device\n",
    "        Device on which to run inference (e.g., torch.device(\"cuda\") or torch.device(\"cpu\")).\n",
    "    mode\n",
    "        Valid values:\n",
    "        - \"per_sample\": standard RMSE over all individual predictions.\n",
    "        - \"batch_mean\": RMSE over per-batch mean predictions vs. per-batch mean targets.\n",
    "    verbose\n",
    "        If True, prints predictions and targets per batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rmse\n",
    "        The computed root mean squared error as a float.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            users = batch[\"users\"].to(device)\n",
    "            items = batch[\"items\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            preds = model(users, items)\n",
    "            trues = targets.view(targets.size(0), -1).to(torch.float32)\n",
    "\n",
    "            if mode == \"per_sample\":\n",
    "                # Standard RMSE across all individual samples\n",
    "                pred_list += preds.view(-1).cpu().tolist()\n",
    "                true_list += trues.view(-1).cpu().tolist()\n",
    "\n",
    "            elif mode == \"batch_mean\":\n",
    "                # RMSE of per-batch averages\n",
    "                batch_pred_mean = preds.mean().item()\n",
    "                batch_true_mean = trues.mean().item()\n",
    "                pred_list.append(batch_pred_mean)\n",
    "                true_list.append(batch_true_mean)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown mode: {mode}. Use 'per_sample' or 'batch_mean'.\"\n",
    "                )\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Predictions: {preds}\")\n",
    "                print(f\"Targets: {trues}\")\n",
    "\n",
    "    rmse = root_mean_squared_error(true_list, pred_list)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard, accurate RMSE over all individual ratings\n",
    "rmse_per_sample = compute_rmse(model, test_loader, device, mode=\"per_sample\")\n",
    "print(\"Per-sample RMSE: {}\".format(np.round(rmse_per_sample, 4)))\n",
    "\n",
    "# RMSE over per-batch means (coarser aggregate)\n",
    "rmse_batch_mean = compute_rmse(model, test_loader, device, mode=\"batch_mean\")\n",
    "print(\"Batch-sample RMSE: {}\".format(np.round(rmse_batch_mean, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6a5d9",
   "metadata": {},
   "source": [
    "**Evaluation Loop w/ Recall & Precision**\n",
    "\n",
    "Recall@k = # of recommended items @k that are relevant / total # of relevant items\n",
    "\n",
    "Precision@k = # of recommended items @k that are relevant / # of recommended items @k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_user_predictions(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    verbose: bool = False,\n",
    ") -> Dict[int, List[Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Run the model on the test set and collect predicted and true targets per user.\n",
    "\n",
    "    Each batch from `test_loader` is expected to be a dict with keys:\n",
    "    - \"users\": user IDs\n",
    "    - \"items\": item IDs\n",
    "    - \"targets\": true target values (e.g., ratings, clicks, scores, etc.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Trained PyTorch model that takes (users, items) and outputs predictions.\n",
    "    test_loader\n",
    "        DataLoader yielding test batches as dicts with \"users\", \"items\", \"targets\".\n",
    "    device\n",
    "        Device on which to run inference (e.g., torch.device(\"cuda\") or torch.device(\"cpu\")).\n",
    "    verbose\n",
    "        If True, prints predictions and targets per batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    user_pred_true\n",
    "        A dictionary mapping each user_id (int) to a list of (predicted_value, true_value)\n",
    "        tuples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_pred_true: DefaultDict[int, List[Tuple[float, float]]] = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            users = batch[\"users\"].to(device)\n",
    "            items = batch[\"items\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            pred_target = model(users, items)\n",
    "            true_target = targets.view(targets.size(0), -1).to(torch.float32)\n",
    "\n",
    "            for idx in range(len(users)):\n",
    "                user_id = users[idx].item()\n",
    "                item_id = items[idx].item()\n",
    "                pred = pred_target[idx][0].item()\n",
    "                true = true_target[idx][0].item()\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"{}, {}, {}, {}\".format(user_id, item_id, pred, true))\n",
    "                user_pred_true[user_id].append((pred, true))\n",
    "\n",
    "    return user_pred_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall_at_k(\n",
    "    user_pred_true: Dict[int, List[Tuple[float, float]]],\n",
    "    k: int = 10,\n",
    "    threshold: float = 3.5,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[Dict[int, float], Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    Compute per-user Precision@K and Recall@K from predicted and true targets.\n",
    "\n",
    "    The `user_pred_true` mapping is expected to store, for each user, a list of\n",
    "    (predicted_value, true_value) tuples. Predicted values are used to rank items,\n",
    "    and both predicted and true values are compared to a relevance threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pred_true\n",
    "        A dictionary mapping each user_id (int) to a list of (predicted_value, true_value)\n",
    "        tuples.\n",
    "    k\n",
    "        The cutoff rank K for Precision@K and Recall@K.\n",
    "    threshold\n",
    "        Relevance threshold. Predicted/true values greater than or equal to this are\n",
    "        considered relevant.\n",
    "    verbose\n",
    "        If True, prints per-user counts used in the metric computations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precisions\n",
    "        A dictionary mapping each user_id (int) to its Precision@K.\n",
    "    recalls\n",
    "        A dictionary mapping each user_id (int) to its Recall@K.\n",
    "    \"\"\"\n",
    "    precisions: Dict[int, float] = {}\n",
    "    recalls: Dict[int, float] = {}\n",
    "\n",
    "    for user_id, user_targets in user_pred_true.items():\n",
    "        # Sort user targets by predicted value (descending)\n",
    "        sorted_targets = sorted(user_targets, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of actually relevant items\n",
    "        n_rel = sum(true_r >= threshold for (_, true_r) in sorted_targets)\n",
    "\n",
    "        # Number of recommended items that are predicted relevant within top-K\n",
    "        top_k = sorted_targets[:k]\n",
    "        n_rec_k = sum(pred_r >= threshold for (pred_r, _) in top_k)\n",
    "\n",
    "        # Number of recommended items that are predicted relevant AND actually relevant\n",
    "        # within top-K\n",
    "        n_rec_rel_k = sum(\n",
    "            (pred_r >= threshold) and (true_r >= threshold)\n",
    "            for (pred_r, true_r) in top_k\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"user_id: {}; n_rel: {}; n_rec_k: {}; n_rec_rel_k: {}\".format(\n",
    "                    user_id, n_rel, n_rec_k, n_rec_rel_k\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant.\n",
    "        precisions[user_id] = n_rec_rel_k / n_rec_k if n_rec_k != 0 else 0.0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended.\n",
    "        recalls[user_id] = n_rec_rel_k / n_rel if n_rel != 0 else 0.0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ebb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 3, 5, 10, 20, 50, 100]\n",
    "THRESHOLD = 3.5\n",
    "user_pred_true = collect_user_predictions(model, test_loader, device)\n",
    "\n",
    "for k in K:\n",
    "\n",
    "    precisions, recalls = compute_precision_recall_at_k(\n",
    "        user_pred_true, k=k, threshold=THRESHOLD\n",
    "    )\n",
    "\n",
    "    total_precision = sum(precision for precision in precisions.values()) / len(\n",
    "        precisions\n",
    "    )\n",
    "    total_recall = sum(recall for recall in recalls.values()) / len(recalls)\n",
    "\n",
    "    print(\"Precision @ {}: {}\".format(k, np.round(total_precision, 4)))\n",
    "    print(\"Recall @ {}: {}\".format(k, np.round(total_recall, 4)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
